{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e664aa37"
      },
      "source": [
        "Splitting the original code cell into multiple cells, each with print statements for better readability."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "756611ff"
      },
      "source": [
        "import os\n",
        "import random\n",
        "from typing import List, Tuple, Dict, Any\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "TRAIN_SIZE = 25000\n",
        "TEST_SIZE = 5000\n",
        "BATCH_SIZE = 512\n",
        "EPOCHS = 300\n",
        "LR = 0.001\n",
        "DROPOUT_P = 0.4\n",
        "HIDDEN_UNITS = 50\n",
        "\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "np.random.seed(RANDOM_SEED)\n",
        "random.seed(RANDOM_SEED)\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def load_income_csv(path: str = \"income.csv\") -> pd.DataFrame:\n",
        "    if not os.path.exists(path):\n",
        "        raise FileNotFoundError(f\"{path} not found. Please place income.csv in the working directory.\")\n",
        "    df = pd.read_csv(path)\n",
        "    return df\n",
        "\n",
        "def preprocess_dataframe(df: pd.DataFrame) -> Tuple[pd.DataFrame, List[str], List[str], str, Dict[str, List[str]]]:\n",
        "    \"\"\"\n",
        "    Separates categorical and continuous columns, encodes categoricals to integer codes.\n",
        "    Returns transformed dataframe, list of categorical columns, continuous columns, label column name, and category mapping.\n",
        "    \"\"\"\n",
        "    label_col = None\n",
        "    for cand in [\"income\", \"Income\", \"target\", \"label\", \"SalStat\"]:\n",
        "        if cand in df.columns:\n",
        "            label_col = cand\n",
        "            break\n",
        "    if label_col is None:\n",
        "        raise ValueError(\"Label column not found. Make sure a suitable label column ('income', 'Income', 'target', 'label', or 'SalStat') exists in the CSV.\")\n",
        "\n",
        "    categorical_cols = df.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
        "    if label_col in categorical_cols:\n",
        "        categorical_cols.remove(label_col)\n",
        "\n",
        "    continuous_cols = df.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
        "    if label_col in continuous_cols:\n",
        "        continuous_cols.remove(label_col)\n",
        "\n",
        "    for c in categorical_cols + [label_col]:\n",
        "        if c in df.columns and df[c].dtype == object:\n",
        "            df[c] = df[c].str.strip()\n",
        "\n",
        "    df = df.copy()\n",
        "    df[label_col] = df[label_col].map(lambda x: 1 if str(x).strip().startswith('>') else 0)\n",
        "\n",
        "    category_mapping = {}\n",
        "    for col in categorical_cols:\n",
        "        df[col] = pd.Categorical(df[col])\n",
        "        category_mapping[col] = list(df[col].cat.categories)\n",
        "        df[col] = df[col].cat.codes.astype('int64')\n",
        "\n",
        "    before = len(df)\n",
        "    df = df.dropna().reset_index(drop=True)\n",
        "    after = len(df)\n",
        "    if after < before:\n",
        "        print(f\"Dropped {before - after} rows containing NaNs during preprocessing.\")\n",
        "\n",
        "    return df, categorical_cols, continuous_cols, label_col, category_mapping\n",
        "\n",
        "class CensusTabularDataset(Dataset):\n",
        "    def __init__(self, cat_tensor: torch.LongTensor, cont_tensor: torch.FloatTensor, labels: torch.LongTensor):\n",
        "        self.cat = cat_tensor\n",
        "        self.cont = cont_tensor\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.cat[idx], self.cont[idx], self.labels[idx]\n",
        "\n",
        "class TabularModel(nn.Module):\n",
        "    def __init__(self, categorical_cardinalities: List[int], continuous_size: int,\n",
        "                 emb_drop_p: float = 0.0, hidden_units: int = 50, dropout_p: float = 0.4):\n",
        "        super().__init__()\n",
        "        self.embeddings = nn.ModuleList()\n",
        "        self.embedding_output_dim = 0\n",
        "        for card in categorical_cardinalities:\n",
        "            emb_size = min(50, (card + 1) // 2)\n",
        "            self.embeddings.append(nn.Embedding(card, emb_size))\n",
        "            self.embedding_output_dim += emb_size\n",
        "\n",
        "        self.bn_cont = nn.BatchNorm1d(continuous_size) if continuous_size > 0 else None\n",
        "\n",
        "        input_dim = self.embedding_output_dim + (continuous_size if continuous_size > 0 else 0)\n",
        "\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_units)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "        self.output = nn.Linear(hidden_units, 2)\n",
        "\n",
        "    def forward(self, x_cat, x_cont):\n",
        "        if len(self.embeddings) > 0:\n",
        "            emb_outs = []\n",
        "            for i, emb in enumerate(self.embeddings):\n",
        "                emb_outs.append(emb(x_cat[:, i]))\n",
        "            x = torch.cat(emb_outs, 1)\n",
        "        else:\n",
        "            x = torch.tensor([], device=x_cont.device)\n",
        "\n",
        "        if x_cont is not None and x_cont.shape[1] > 0:\n",
        "            if self.bn_cont is not None:\n",
        "                cont = self.bn_cont(x_cont)\n",
        "            else:\n",
        "                cont = x_cont\n",
        "            if x.numel() > 0:\n",
        "                x = torch.cat([x, cont], 1)\n",
        "            else:\n",
        "                x = cont\n",
        "\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.output(x)\n",
        "        return x\n",
        "\n",
        "def prepare_tensors(df: pd.DataFrame, categorical_cols: List[str], continuous_cols: List[str], label_col: str,\n",
        "                    scaler: StandardScaler = None) -> Tuple[torch.LongTensor, torch.FloatTensor, torch.LongTensor, StandardScaler]:\n",
        "    cat_arr = df[categorical_cols].values.astype('int64') if len(categorical_cols) > 0 else np.zeros((len(df), 0), dtype='int64')\n",
        "    cat_tensor = torch.from_numpy(cat_arr).long()\n",
        "\n",
        "    cont_arr = df[continuous_cols].values.astype('float32') if len(continuous_cols) > 0 else np.zeros((len(df), 0), dtype='float32')\n",
        "    if scaler is None and len(continuous_cols) > 0:\n",
        "        scaler = StandardScaler()\n",
        "        cont_arr = scaler.fit_transform(cont_arr)\n",
        "    elif len(continuous_cols) > 0:\n",
        "        cont_arr = scaler.transform(cont_arr)\n",
        "    cont_tensor = torch.from_numpy(cont_arr.astype('float32'))\n",
        "\n",
        "    labels = torch.from_numpy(df[label_col].values.astype('int64'))\n",
        "\n",
        "    return cat_tensor, cont_tensor, labels, scaler\n",
        "\n",
        "def train_epoch(model: nn.Module, dataloader: DataLoader, criterion, optimizer) -> float:\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for x_cat, x_cont, y in dataloader:\n",
        "        x_cat = x_cat.to(DEVICE)\n",
        "        x_cont = x_cont.to(DEVICE)\n",
        "        y = y.to(DEVICE)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(x_cat, x_cont)\n",
        "        loss = criterion(outputs, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * x_cat.size(0)\n",
        "\n",
        "    return running_loss / len(dataloader.dataset)\n",
        "\n",
        "def eval_model(model: nn.Module, dataloader: DataLoader, criterion) -> Tuple[float, float]:\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for x_cat, x_cont, y in dataloader:\n",
        "            x_cat = x_cat.to(DEVICE)\n",
        "            x_cont = x_cont.to(DEVICE)\n",
        "            y = y.to(DEVICE)\n",
        "\n",
        "            outputs = model(x_cat, x_cont)\n",
        "            loss = criterion(outputs, y)\n",
        "            running_loss += loss.item() * x_cat.size(0)\n",
        "\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "            correct += (preds == y).sum().item()\n",
        "            total += y.size(0)\n",
        "\n",
        "    return running_loss / len(dataloader.dataset), correct / total\n",
        "\n",
        "def predict_instance(model: nn.Module, instance: Dict[str, Any], categorical_cols: List[str], continuous_cols: List[str],\n",
        "                     category_mapping: Dict[str, List[str]], scaler: StandardScaler) -> str:\n",
        "    \"\"\"\n",
        "    instance: dict mapping column name -> value (strings for categorical, numbers for numeric)\n",
        "    Returns label string.\n",
        "    \"\"\"\n",
        "    row = {}\n",
        "    for c in categorical_cols + continuous_cols:\n",
        "        if c in instance:\n",
        "            row[c] = instance[c]\n",
        "        else:\n",
        "            raise ValueError(f\"Missing column {c} in the instance. Provide all categorical and continuous columns.\")\n",
        "    df_row = pd.DataFrame([row])\n",
        "\n",
        "    for col in categorical_cols:\n",
        "        cats = category_mapping[col]\n",
        "        try:\n",
        "            code = cats.index(df_row.loc[0, col])\n",
        "        except ValueError:\n",
        "            code = 0\n",
        "        df_row[col] = code\n",
        "\n",
        "    if len(continuous_cols) > 0:\n",
        "        cont = scaler.transform(df_row[continuous_cols].values.astype('float32'))\n",
        "        cont_tensor = torch.from_numpy(cont.astype('float32')).to(DEVICE)\n",
        "    else:\n",
        "        cont_tensor = torch.zeros((1, 0)).to(DEVICE)\n",
        "\n",
        "    if len(categorical_cols) > 0:\n",
        "        cat_arr = df_row[categorical_cols].values.astype('int64')\n",
        "        cat_tensor = torch.from_numpy(cat_arr).long().to(DEVICE)\n",
        "    else:\n",
        "        cat_tensor = torch.zeros((1, 0)).long().to(DEVICE)\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        out = model(cat_tensor, cont_tensor)\n",
        "        pred = torch.argmax(out, dim=1).item()\n",
        "    return \">50K\" if pred == 1 else \"<=50K\""
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7c8560b",
        "outputId": "cf762f28-0004-45f7-c014-c8daf5def9d7"
      },
      "source": [
        "print(f\"Using device: {DEVICE}\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "179c8748",
        "outputId": "ee29f305-76ee-42cd-97b4-4fbe51766b4a"
      },
      "source": [
        "# 1. Load\n",
        "df = load_income_csv(\"income.csv\")\n",
        "print(\"Loaded dataframe shape:\", df.shape)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded dataframe shape: (31978, 13)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65750de0",
        "outputId": "0a6f3f24-19fe-457f-fa5f-1ac9b24a96f6"
      },
      "source": [
        "# Print column names for inspection\n",
        "print(\"Columns in the DataFrame:\", df.columns.tolist())"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns in the DataFrame: ['age', 'JobType', 'EdType', 'maritalstatus', 'occupation', 'relationship', 'race', 'gender', 'capitalgain', 'capitalloss', 'hoursperweek', 'nativecountry', 'SalStat']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4278fbd5",
        "outputId": "afc64d59-faed-485e-abbf-4cc98ccea665"
      },
      "source": [
        "# 2. Preprocess\n",
        "df_proc, categorical_cols, continuous_cols, label_col, category_mapping = preprocess_dataframe(df)\n",
        "print(\"Categorical cols:\", categorical_cols)\n",
        "print(\"Continuous cols:\", continuous_cols)\n",
        "print(\"Label col:\", label_col)\n",
        "\n",
        "if len(df_proc) < TRAIN_SIZE + TEST_SIZE:\n",
        "    raise ValueError(f\"Dataset has only {len(df_proc)} rows after cleaning but requires {TRAIN_SIZE+TEST_SIZE} rows.\")\n",
        "\n",
        "# Shuffle and split to exact sizes\n",
        "df_proc = df_proc.sample(frac=1, random_state=RANDOM_SEED).reset_index(drop=True)\n",
        "df_train = df_proc.iloc[:TRAIN_SIZE].reset_index(drop=True)\n",
        "df_test = df_proc.iloc[TRAIN_SIZE:TRAIN_SIZE+TEST_SIZE].reset_index(drop=True)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Categorical cols: ['JobType', 'EdType', 'maritalstatus', 'occupation', 'relationship', 'race', 'gender', 'nativecountry']\n",
            "Continuous cols: ['age', 'capitalgain', 'capitalloss', 'hoursperweek']\n",
            "Label col: SalStat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbf811e6"
      },
      "source": [
        "# 3. Prepare tensors (scaler fitted on train)\n",
        "cat_train, cont_train, y_train, scaler = prepare_tensors(df_train, categorical_cols, continuous_cols, label_col, scaler=None)\n",
        "cat_test, cont_test, y_test, _ = prepare_tensors(df_test, categorical_cols, continuous_cols, label_col, scaler=scaler)\n",
        "\n",
        "# Build datasets and dataloaders\n",
        "train_dataset = CensusTabularDataset(cat_train, cont_train, y_train)\n",
        "test_dataset = CensusTabularDataset(cat_test, cont_test, y_test)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# 4. Model\n",
        "categorical_cardinalities = [len(category_mapping[c]) for c in categorical_cols]\n",
        "model = TabularModel(categorical_cardinalities, len(continuous_cols), emb_drop_p=0.0, hidden_units=HIDDEN_UNITS, dropout_p=DROPOUT_P)\n",
        "model.to(DEVICE)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LR)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "092728d1",
        "outputId": "5cbc23e7-ca6d-4b5a-a23c-7d451d1158a7"
      },
      "source": [
        "# 5. Train\n",
        "best_test_acc = 0.0\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    train_loss = train_epoch(model, train_loader, criterion, optimizer)\n",
        "    test_loss, test_acc = eval_model(model, test_loader, criterion)\n",
        "\n",
        "    if epoch % 10 == 0 or epoch == 1:\n",
        "        print(f\"Epoch {epoch:03d} | Train Loss: {train_loss:.4f} | Test Loss: {test_loss:.4f} | Test Acc: {test_acc*100:.2f}%\")\n",
        "\n",
        "    # optional: save best\n",
        "    if test_acc > best_test_acc:\n",
        "        best_test_acc = test_acc\n",
        "        torch.save({'model_state_dict': model.state_dict(), 'scaler': scaler, 'categorical_cols': categorical_cols, 'continuous_cols': continuous_cols, 'category_mapping': category_mapping}, 'best_tabular_model.pth')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | Train Loss: 0.1888 | Test Loss: 0.0146 | Test Acc: 100.00%\n",
            "Epoch 010 | Train Loss: 0.0003 | Test Loss: 0.0001 | Test Acc: 100.00%\n",
            "Epoch 020 | Train Loss: 0.0001 | Test Loss: 0.0000 | Test Acc: 100.00%\n",
            "Epoch 030 | Train Loss: 0.0000 | Test Loss: 0.0000 | Test Acc: 100.00%\n",
            "Epoch 040 | Train Loss: 0.0000 | Test Loss: 0.0000 | Test Acc: 100.00%\n",
            "Epoch 050 | Train Loss: 0.0000 | Test Loss: 0.0000 | Test Acc: 100.00%\n",
            "Epoch 060 | Train Loss: 0.0000 | Test Loss: 0.0000 | Test Acc: 100.00%\n",
            "Epoch 070 | Train Loss: 0.0000 | Test Loss: 0.0000 | Test Acc: 100.00%\n",
            "Epoch 080 | Train Loss: 0.0000 | Test Loss: 0.0000 | Test Acc: 100.00%\n",
            "Epoch 090 | Train Loss: 0.0000 | Test Loss: 0.0000 | Test Acc: 100.00%\n",
            "Epoch 100 | Train Loss: 0.0000 | Test Loss: 0.0000 | Test Acc: 100.00%\n",
            "Epoch 110 | Train Loss: 0.0000 | Test Loss: 0.0000 | Test Acc: 100.00%\n",
            "Epoch 120 | Train Loss: 0.0000 | Test Loss: 0.0000 | Test Acc: 100.00%\n",
            "Epoch 130 | Train Loss: 0.0000 | Test Loss: 0.0000 | Test Acc: 100.00%\n",
            "Epoch 140 | Train Loss: 0.0000 | Test Loss: 0.0000 | Test Acc: 100.00%\n",
            "Epoch 150 | Train Loss: 0.0000 | Test Loss: 0.0000 | Test Acc: 100.00%\n",
            "Epoch 160 | Train Loss: 0.0000 | Test Loss: 0.0000 | Test Acc: 100.00%\n",
            "Epoch 170 | Train Loss: 0.0000 | Test Loss: 0.0000 | Test Acc: 100.00%\n",
            "Epoch 180 | Train Loss: 0.0000 | Test Loss: 0.0000 | Test Acc: 100.00%\n",
            "Epoch 190 | Train Loss: 0.0000 | Test Loss: 0.0000 | Test Acc: 100.00%\n",
            "Epoch 200 | Train Loss: 0.0000 | Test Loss: 0.0000 | Test Acc: 100.00%\n",
            "Epoch 210 | Train Loss: 0.0000 | Test Loss: 0.0000 | Test Acc: 100.00%\n",
            "Epoch 220 | Train Loss: 0.0000 | Test Loss: 0.0000 | Test Acc: 100.00%\n",
            "Epoch 230 | Train Loss: 0.0000 | Test Loss: 0.0000 | Test Acc: 100.00%\n",
            "Epoch 240 | Train Loss: 0.0000 | Test Loss: 0.0000 | Test Acc: 100.00%\n",
            "Epoch 250 | Train Loss: 0.0000 | Test Loss: 0.0000 | Test Acc: 100.00%\n",
            "Epoch 260 | Train Loss: 0.0000 | Test Loss: 0.0000 | Test Acc: 100.00%\n",
            "Epoch 270 | Train Loss: 0.0000 | Test Loss: 0.0000 | Test Acc: 100.00%\n",
            "Epoch 280 | Train Loss: 0.0000 | Test Loss: 0.0000 | Test Acc: 100.00%\n",
            "Epoch 290 | Train Loss: 0.0000 | Test Loss: 0.0000 | Test Acc: 100.00%\n",
            "Epoch 300 | Train Loss: 0.0000 | Test Loss: 0.0000 | Test Acc: 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "478bfb8d",
        "outputId": "07a72a70-e976-415b-c3da-d4595adef628"
      },
      "source": [
        "# Final evaluation\n",
        "test_loss, test_acc = eval_model(model, test_loader, criterion)\n",
        "print(\"\\nTraining complete.\")\n",
        "print(f\"Final Test Loss: {test_loss:.4f}\")\n",
        "print(f\"Final Test Accuracy: {test_acc*100:.2f}%\")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training complete.\n",
            "Final Test Loss: 0.0000\n",
            "Final Test Accuracy: 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66e725a7",
        "outputId": "d03b2423-8b3d-4d0d-d9ec-e172aa52038e"
      },
      "source": [
        "# Example of predict_instance on a sample from test set (convert codes back to category values for human-readable example)\n",
        "# Construct a sample dict from the first test row (but map categorical numeric codes back to category labels)\n",
        "sample_row = df_test.iloc[0]\n",
        "sample_input = {}\n",
        "for c in categorical_cols:\n",
        "    cats = category_mapping[c]\n",
        "    code = int(sample_row[c])\n",
        "    # protect against out-of-range\n",
        "    val = cats[code] if 0 <= code < len(cats) else cats[0]\n",
        "    sample_input[c] = val\n",
        "for c in continuous_cols:\n",
        "    sample_input[c] = float(sample_row[c])\n",
        "\n",
        "print('\\nSample input (human readable):')\n",
        "print(sample_input)\n",
        "pred_label = predict_instance(model, sample_input, categorical_cols, continuous_cols, category_mapping, scaler)\n",
        "print('Model prediction for sample:', pred_label)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sample input (human readable):\n",
            "{'JobType': 'Private', 'EdType': 'Masters', 'maritalstatus': 'Never-married', 'occupation': 'Adm-clerical', 'relationship': 'Not-in-family', 'race': 'White', 'gender': 'Female', 'nativecountry': 'United-States', 'age': 86.0, 'capitalgain': 0.0, 'capitalloss': 0.0, 'hoursperweek': 40.0}\n",
            "Model prediction for sample: <=50K\n"
          ]
        }
      ]
    }
  ]
}